#!/bin/bash
# Universal Local Model Starter with MLX + LiteLLM

set -e

CONFIG_FILE="$1"
PROXY_PORT=18080

# Validate arguments
if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <config-file>"
    echo "Example: $0 configs/local-glm-9b.yaml"
    exit 1
fi

if [ ! -f "$CONFIG_FILE" ]; then
    echo "âŒ Config file not found: $CONFIG_FILE"
    exit 1
fi

# Extract model info from config
MODEL=$(grep -A2 "mlx_config:" "$CONFIG_FILE" | grep "model:" | cut -d'"' -f2)
MLX_PORT=$(grep "port:" "$CONFIG_FILE" | cut -d' ' -f2 | head -1)
MODEL_NAME=$(grep "name:" "$CONFIG_FILE" | cut -d'"' -f2)

if [ -z "$MODEL" ] || [ -z "$MLX_PORT" ]; then
    echo "âŒ Invalid local config file: $CONFIG_FILE"
    echo "Missing model or port configuration"
    exit 1
fi

echo "ğŸš€ Starting $MODEL_NAME..."
echo "ğŸ“ Model: $MODEL"
echo "ğŸ“ MLX Port: $MLX_PORT"
echo "ğŸ“ Proxy Port: $PROXY_PORT"
echo ""

# Check if ports are available
if lsof -i:$PROXY_PORT > /dev/null 2>&1; then
    echo "âš ï¸  Proxy port $PROXY_PORT already in use"
    exit 1
fi

if lsof -i:$MLX_PORT > /dev/null 2>&1; then
    echo "âš ï¸  MLX port $MLX_PORT already in use"
    exit 1
fi

# Set up environment - use current python3 configuration

# Check if model exists locally
echo "ğŸ” Checking model availability: $MODEL"
MODEL_EXISTS=$(python3 -c "
import os
try:
    cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
    if os.path.exists(cache_dir):
        model_cache_exists = any('$MODEL'.replace('/', '--') in d for d in os.listdir(cache_dir) if os.path.isdir(os.path.join(cache_dir, d)))
        print('true' if model_cache_exists else 'false')
    else:
        print('false')
except:
    print('false')
")

if [ "$MODEL_EXISTS" = "true" ]; then
    echo "âœ… Model found in local cache"
    echo "ğŸ“¡ Starting MLX server in background on port $MLX_PORT..."
    
    # Start MLX server in background
    nohup python3 -c "
from mlx_lm.server import main
import sys
sys.argv = ['mlx_lm.server', '--model', '$MODEL', '--port', '$MLX_PORT']
main()
" > "mlx-server.log" 2>&1 &
    MLX_PID=$!
    
    # Wait for MLX server to start
    sleep 3
    
else
    echo "ğŸ“¦ Model not found locally - will download first"
    echo "ğŸ“¡ Starting MLX server in foreground (showing download progress)..."
    echo ""
    
    # Start MLX server in foreground until it's running
    python3 -c "
from mlx_lm.server import main
import sys
sys.argv = ['mlx_lm.server', '--model', '$MODEL', '--port', '$MLX_PORT']
main()
" &
    MLX_PID=$!
    
    # Wait for server to be responding (no timeout - model download can take time)
    echo ""
    echo "â³ Waiting for server to be ready (downloading model if needed)..."
    while true; do
        if curl -s http://localhost:$MLX_PORT/v1/models > /dev/null 2>&1; then
            echo "âœ… MLX server is ready! Moving to background..."
            break
        fi
        sleep 10
    done
fi

# Start LiteLLM Proxy
echo "ğŸ”„ Starting LiteLLM Proxy on port $PROXY_PORT..."
nohup litellm --config "$CONFIG_FILE" --port $PROXY_PORT > "litellm-proxy.log" 2>&1 &
PROXY_PID=$!

# Save PIDs and config
echo "$MLX_PID" > mlx-server.pid
echo "$PROXY_PID" > litellm-proxy.pid
echo "$CONFIG_FILE" > current-config.txt

echo ""
echo "âœ… Services started successfully!"
echo "ğŸ“ MLX Server PID: $MLX_PID (port $MLX_PORT)"
echo "ğŸ“ LiteLLM Proxy PID: $PROXY_PID (port $PROXY_PORT)"
echo "ğŸ“„ MLX Logs: mlx-server.log"
echo "ğŸ“„ Proxy Logs: litellm-proxy.log"
echo "ğŸŒ Claude Code URL: http://localhost:$PROXY_PORT"
echo ""
echo "To stop: ./scripts/stop.sh"
echo "To check status: ./scripts/status.sh"