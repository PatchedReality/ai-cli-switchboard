# Local DeepSeek V2.5 Configuration (MLX)
model_list:
  - model_name: deepseek-v2.5-local
    litellm_params:
      model: openai/mlx-community/DeepSeek-V2.5-MLX-AQ4_1_64
      api_base: http://localhost:18081/v1
      api_key: dummy-key

litellm_settings:
  success_callback: []
  failure_callback: []
  request_timeout: 120
  drop_params: true

general_settings:
  master_key: dummy-key
  completion_model: deepseek-v2.5-local

# MLX Server Configuration  
mlx_config:
  model: "mlx-community/DeepSeek-V2.5-MLX-AQ4_1_64"
port: 18081  # This line needs to be unindented for start-local.sh parsing

# Model info
model_info:
  name: "DeepSeek-V2.5 (Local MLX)"
  type: "local"
  provider: "MLX"
  context_length: 128000
  parameters: "36.9B (quantized)"
  capabilities: ["coding", "reasoning", "tool_calling"]
  memory_requirement: "~25-30GB RAM"
  cost: "Free (local inference)"

# Alias configuration
alias_config:
  alias_name: "claude-local-deepseek"
  runner_type: "local_mlx"
  description: "DeepSeek-V2.5 (25-30GB, reasoning)"