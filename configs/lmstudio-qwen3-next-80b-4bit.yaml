# LM Studio Qwen3 Next 80B 4-bit Configuration
# Model URL: https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit
model_list:
  - model_name: qwen3-next-80b-4bit-lms
    litellm_params:
      model: openai/qwen3-next-80b-4bit
      api_base: http://127.0.0.1:1234/v1
      api_key: dummy-key

litellm_settings:
  success_callback: []
  failure_callback: []
  request_timeout: 300
  drop_params: false

general_settings:
  master_key: dummy-key
  completion_model: qwen3-next-80b-4bit-lms

# LM Studio Configuration
lmstudio_config:
  model_key: "mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit"
  server_port: 1234
  gpu_offload: "max"
  context_length: 131072

# Model info
model_info:
  name: "Qwen3 Next 80B 4-bit (LM Studio)"
  type: "local"
  provider: "LM Studio"
  context_length: 131072
  parameters: "80B total (3B active)"
  quantization: "4-bit"
  architecture: "Hybrid attention with high-sparsity MoE"
  capabilities: ["general", "coding", "reasoning", "tool_calling"]
  memory_requirement: "~45-50GB VRAM (4-bit quantization)"
  cost: "Free (local inference)"
  performance: "80B model with 4-bit quantization for better reasoning than 3-bit"
  notes: "4-bit quantized version should provide better tool usage while fitting in 48GB VRAM"

# Alias configuration
alias_config:
  alias_name: "claude-lmstudio-qwen3-next-80b-4bit"
  runner_type: "local_lmstudio"
  description: "Qwen3 Next 80B 4-bit via LM Studio (45-50GB, better tool usage)"